{
  "type": "Self Speculative Decoding",
  "task": "xsum",
  "n_shot": 1,
  "length": 50,
  "rouge-1": 0.21144149506067617,
  "rouge-2": 0.053302280043358116,
  "rouge-l": 0.14987963983764221,
  "rouge-lsum": 0.1491694482504638,
  "elapsed_time": 171.14845442771912,
  "generated_tokens": 5162,
  "model_memory": 5.984714508056641,
  "peak_memory": 7.722429275512695,
  "inputs": {
    "model_id": "meta-llama/Llama-3.2-3B-Instruct",
    "num_offload_layers": 0,
    "search_iteration": 1000,
    "csv_filename": "config/llama-3.2-3b/motivation/offload_0-search_1000/searching.csv",
    "json_filename": "config/llama-3.2-3b/motivation/offload_0-search_1000/skipped_layers.json",
    "skip_attn_layers": [
      1,
      3,
      5,
      6,
      8,
      9,
      12,
      18,
      20,
      22,
      27
    ],
    "skip_mlp_layers": [
      1,
      3,
      5,
      6,
      7,
      8,
      9,
      13,
      17,
      22,
      23
    ]
  }
}